{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "008e690e-6886-4a04-b942-901554e4cc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U langgraph -q\n",
    "!pip install langchain -q\n",
    "!pip install ollama -q\n",
    "\n",
    "!pip install pymongo -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b960610-6986-44a0-8c68-d3a3c90b3b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langgraph.graph import END, MessageGraph\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "from pymongo import MongoClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6f19696-d39a-425e-97b4-d3da711b03ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient('localhost', 27017)\n",
    "database = client['LangChain_Intento4']\n",
    "collection = database['lenguajes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d134181-fdae-4a98-a0f7-0607ee51dcc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aqui debe llegar user_name y user_email\n",
    "# esto buscará el usuario según el email\n",
    "\n",
    "def function1(state): #buscar usuario\n",
    "    messages = state['messages']\n",
    "    agent_response = messages[-1]\n",
    "    documents = collection.find({'email': user_email})\n",
    "    user_agenda = []\n",
    "    for document in documents:\n",
    "        user_agenda.append(document)\n",
    "        state['messages'].append(user_agenda)\n",
    "        return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ae4c7fb-b2a5-49dc-9b01-751132232688",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_dia = \"15\"\n",
    "user_mes = \"mayo\"\n",
    "user_hora = \"14\"\n",
    "\n",
    "# aqui debe llegar user_dia, user_mes y user_hora\n",
    "\n",
    "def function2(state): #agendar cita\n",
    "\n",
    "    messages = state['messages']\n",
    "\n",
    "    user_cita = user_dia + \" de \" + user_mes + \" del 2024 a las \" + user_hora + \" horas\"\n",
    "    agenda_local = collection.find({'cita': user_cita})\n",
    "    repetidas = []\n",
    "\n",
    "    for agenda_citas in agenda_local:\n",
    "        repetidas.append(agenda_citas)\n",
    "    if len(repetidas)>0:\n",
    "        print(\"agenda otra hora\")\n",
    "    else:\n",
    "        print(\"añadir cita\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fa43bb-8419-4cfe-bd42-0bf7626376b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm1 = Ollama(model='llama3', temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e7f008-4fb4-40ab-aed1-3635e9d35cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Cuanto es 1 + 1?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6543966-780e-4521-a496-bfb0fe3ca316",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = MessageGraph()\n",
    "\n",
    "graph.add_node(\"oracle\", llm1)\n",
    "graph.add_edge(\"oracle\", END)\n",
    "\n",
    "graph.set_entry_point(\"oracle\")\n",
    "\n",
    "runnable = graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7cbbe01e-0b80-49ef-8a1a-78704bb47834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Ingrese mensaje:  Hola, me llamo Osvaldo\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['System: eres un asistente chatbot que habla español', 'AI: Hola! Soy un chatbot que puede ayudarte en español. ¿En qué puedo ayudarte hoy? ¿Tienes alguna pregunta o necesitas ayuda con algo en particular? Estoy aquí para escucharte y ayudarte lo mejor que pueda. ¡Hablemos!', 'Human: Hola, me llamo Osvaldo', 'AI: ¡Hola Osvaldo! Me alegra poder hablar contigo. ¿En qué puedo ayudarte hoy? ¿Tienes alguna pregunta o necesitas ayuda con algo en particular? Estoy aquí para escucharte y ayudarte lo mejor que pueda. ¡Hablemos!']\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Ingrese mensaje:  cómo me llamo?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['System: eres un asistente chatbot que habla español', 'AI: Hola! Soy un chatbot que puede ayudarte en español. ¿En qué puedo ayudarte hoy? ¿Tienes alguna pregunta o necesitas ayuda con algo en particular? Estoy aquí para escucharte y ayudarte lo mejor que pueda. ¡Hablemos!', 'Human: Hola, me llamo Osvaldo', 'AI: ¡Hola Osvaldo! Me alegra poder hablar contigo. ¿En qué puedo ayudarte hoy? ¿Tienes alguna pregunta o necesitas ayuda con algo en particular? Estoy aquí para escucharte y ayudarte lo mejor que pueda. ¡Hablemos!', 'Human: cómo me llamo?', \"It seems like you're having a conversation with an AI chatbot in Spanish! Let's summarize what's been said so far:\\n\\n1. The AI introduces itself as a chatbot that can assist in Spanish.\\n2. You, Osvaldo, greet the AI and ask how to refer to yourself.\\n\\nNow it's your turn again! What would you like to say next?\"]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m store\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAI: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m iniciar[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mcontent)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m user_input \u001b[38;5;241m!=\u001b[39m terminar:\n\u001b[1;32m---> 10\u001b[0m     user_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIngrese mensaje: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     11\u001b[0m     store\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHuman: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m user_input)\n\u001b[0;32m     12\u001b[0m     respuesta \u001b[38;5;241m=\u001b[39m runnable\u001b[38;5;241m.\u001b[39minvoke(HumanMessage(\u001b[38;5;28mstr\u001b[39m(store)))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\LangChain_Intento9_OllamaFull\\Lib\\site-packages\\ipykernel\\kernelbase.py:1262\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m   1260\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1261\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[1;32m-> 1262\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_request(\n\u001b[0;32m   1263\u001b[0m     \u001b[38;5;28mstr\u001b[39m(prompt),\n\u001b[0;32m   1264\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent_ident[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshell\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   1265\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_parent(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshell\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1266\u001b[0m     password\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1267\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\LangChain_Intento9_OllamaFull\\Lib\\site-packages\\ipykernel\\kernelbase.py:1305\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1302\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m   1303\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[0;32m   1304\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1305\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1306\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1307\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "terminar = \"bye\"\n",
    "user_input = \"\"\n",
    "store = []\n",
    "inicializador = \"eres un asistente chatbot que habla español\"\n",
    "store.append(\"System: \" + inicializador)\n",
    "iniciar = runnable.invoke(HumanMessage(inicializador))\n",
    "store.append(\"AI: \" + iniciar[-1].content)\n",
    "\n",
    "while user_input != terminar:\n",
    "    user_input = input(\"Ingrese mensaje: \")\n",
    "    store.append(\"Human: \" + user_input)\n",
    "    respuesta = runnable.invoke(HumanMessage(str(store)))\n",
    "    store.append(respuesta[-1].content)\n",
    "    print(store)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ffdbbf-0078-48e5-b873-c4cda771d213",
   "metadata": {},
   "source": [
    "## Conditional Edges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09110a0b-b297-46dc-a95c-e355f287cd44",
   "metadata": {},
   "source": [
    "# Paso 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c61d78-1856-4e44-a37e-731be1777a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# asigna AgentState como un diccionario vacío\n",
    "AgentState = {}\n",
    "\n",
    "# La key de mensajes se asignará como un array vacío. Agregaremos nuevos mensajes a medida que pasemos por los nodos. \n",
    "AgentState[\"messages\"] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ab2dcf-0879-45c7-aeaa-2c5f941f0a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "AgentState"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6045e76b-df3e-4e58-a04d-3ea2232d09b7",
   "metadata": {},
   "source": [
    "##### Nuestro objetivo es que este estado se llene así: {'messages': [HumanMessage, AIMessage, ...]]}\r\n",
    "Además, ahora necesitamos modificar nuestras funciones para pasar información a lo largo del nuevo AgentState.te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e28b76-94de-4020-bd01-60fbbacb5a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def function1(state): #buscar usuario\n",
    "    messages = state['messages']\n",
    "    user_input = messages[-1]\n",
    "    documents = str({\n",
    "  \"nombre\": \"Osvaldo\",\n",
    "  \"correo\": \"osvaldo@email.com\",\n",
    "  \"citacion\": \"miércoles 22 de mayo a las 15:00\",\n",
    "  \"telefono\": \"\"\n",
    "})\n",
    "    complete_query = \"Dime los siguientes datos obtenidos desde una base de datos:\" + documents\n",
    "    respuesta = llm1.invoke(complete_query)\n",
    "    print(respuesta)\n",
    "    print(respuesta[-1])\n",
    "    state['messages'].append(respuesta) # añadiendo la respuesta AIMessage al AgentState\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26c5fae-2f35-47bb-838a-94f80c5e7855",
   "metadata": {},
   "outputs": [],
   "source": [
    "def function2(state): #agendar cita\n",
    "\n",
    "    messages2 = state['messages']\n",
    "    datos_recibidos = messages2[-1]\n",
    "    query_nueva = \"Tienes los siguientes datos: \" + datos_recibidos + \", consulta por los faltantes, y si no faltan, pregunta si quiere agendar una cita nueva\"\n",
    "    respuesta_nueva = llm1.invoke(query_nueva)\n",
    "    print(respuesta_nueva)\n",
    "    print(respuesta_nueva[-1])\n",
    "    state['messages'].append(respuesta_nueva)\n",
    "    return state\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ef9e35-062b-436e-9284-ae5e10f0c42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def function3(state): \n",
    "    messages3 = state['messages']\n",
    "    datos_recibidos2 = messages3[-1]\n",
    "    query_nueva2 = \"Asume que la respuesta fue un sí y pregunta a qué hora la cita nueva\"\n",
    "    respuesta_nueva2 = llm1.invoke(query_nueva2)\n",
    "    print(respuesta_nueva2)\n",
    "    print(respuesta_nueva2[-1])\n",
    "    state['messages'].append(respuesta_nueva2)\n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb1250c-4301-44ff-be68-53254ba61d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import Graph\n",
    "\n",
    "workflow = Graph()\n",
    "\n",
    "\n",
    "workflow.add_node(\"agent\", function1)\n",
    "workflow.add_node(\"tool\", function2)\n",
    "workflow.add_node(\"responder\", function3)\n",
    "\n",
    "workflow.add_edge('agent', 'tool')\n",
    "workflow.add_edge('tool', 'responder')\n",
    "\n",
    "workflow.set_entry_point(\"agent\")\n",
    "workflow.set_finish_point(\"responder\")\n",
    "\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fae9b1-80cd-4f4f-bf27-4f084f0e1f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {\"messages\": [\"what is the temperature in las vegas\"]}\n",
    "app.invoke(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a346ca5d-b0d5-41fb-9337-677ab5a9812e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = {\"messages\": [\"Quiero una cita, me llamo Osvaldo\"]}\n",
    "for output in app.stream(input):\n",
    "    # stream() yields dictionaries with output keyed by node name\n",
    "    for key, value in output.items():\n",
    "        print(f\"Output from node '{key}':\")\n",
    "        print(\"---\")\n",
    "        print(value)\n",
    "    print(\"\\n---\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ac3667-9f1e-4942-b744-e50b0f4429e8",
   "metadata": {},
   "source": [
    "Como notamos que se están agregando muchas cosas al array, podemos hacerlo un poco más fácil con lo siguiente:\r\n",
    "\r\n",
    "```bash\r\n",
    "from typing import TypedDict, Annotated, Sequence\r\n",
    "import operator\r\n",
    "from langchain_core.messages import BaseMessage\r\n",
    "\r\n",
    "\r\n",
    "class AgentState(TypedDict):\r\n",
    "    messages: Annotated[Sequence[BaseMessage], operator.add]seMessage], operator.add]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706ff1c3-1567-48fc-957b-cb69334603db",
   "metadata": {},
   "source": [
    "# Conditonal Edges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f858806f-90b7-4fe0-9fa4-670efb5a3bd8",
   "metadata": {},
   "source": [
    "Podemos hacer que nuestro agente sea más inteligente diciéndole que solo use la herramienta cuando sea necesario, si no simplemente responda al usuario. \n",
    "\n",
    "La forma en que podemos hacer este LangGraph es:\n",
    "1. Vincular una Tool al agente\n",
    "2. Añadirr unaConditional Edgel al agente con la opción de llamar a laTool  o n\n",
    "3. definiendo los criterios para el Conditional Edge sobre cuándo llamar a la Tool. Definiremos una función para esto..\n",
    "\n",
    "Comencemos con la definición de AgentState como se menciona algunas celdas arriba.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849997dd-4820-4d92-8af8-fd16711ffafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated, Sequence\n",
    "import operator\n",
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], operator.add]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7197cdbe-6fe2-44af-bddf-ee89e06e3aab",
   "metadata": {},
   "source": [
    "La herramienta de vinculación con agente (modelo LLM) se simplifica en langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16aaf72-a345-40da-a85a-d06ecf53e8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.utils.function_calling import convert_to_openai_function\n",
    "from langchain_core.utils.function_calling import convert_to_openai_function\n",
    "\n",
    "model = Ollama(temperature=0, model='llama3')\n",
    "functions = [convert_to_openai_function(t) for t in tools]\n",
    "model = model.bind_functions(functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d569289-5394-43a1-90b9-98c3a0006c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "@tool\n",
    "def multiply(first_number: int, second_number: int):\n",
    "    \"\"\"Multiplies two numbers together.\"\"\"\n",
    "    return first_number * second_number\n",
    "\n",
    "model = Ollama(model='llama3',temperature=0)\n",
    "model_with_tools = model.bind_tools([multiply])\n",
    "\n",
    "builder = MessageGraph()\n",
    "\n",
    "builder.add_node(\"oracle\", model_with_tools)\n",
    "\n",
    "tool_node = ToolNode([multiply])\n",
    "builder.add_node(\"multiply\", tool_node)\n",
    "\n",
    "builder.add_edge(\"multiply\", END)\n",
    "\n",
    "builder.set_entry_point(\"oracle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be49dc22-e69c-48a0-b61b-007ca8169aaa",
   "metadata": {},
   "source": [
    "# Intento de memoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1bb5290f-880f-40ee-9918-6c01c3189875",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "54b9ae38-4b4c-4120-86bd-18f0280ccaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "996d7507-c3e9-4699-99e4-ceb7e489c3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ollama -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "537b04f7-2795-4be3-b7bf-fd25b9fc87ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "\n",
    "model = Ollama(model=\"llama3\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "45b1438d-6987-4a11-8a0d-1086c297f29c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"AI: Hi Bob! It's nice to meet you. Is there something I can help you with or would you like to chat?\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "model.invoke([HumanMessage(\"Hi! I'm Bob\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0a7b290f-d7f3-4e17-ac02-6975942b98fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bob! Your name is indeed Bob.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "model.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"Hi! I'm Bob\"),\n",
    "        AIMessage(content=\"Hello Bob! How can I assist you today?\"),\n",
    "        HumanMessage(content=\"What's my name?\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e35a5320-adb9-41e7-84b9-05f383e15d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install langchain_community -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1dbe1be6-aa24-42d6-b0e7-6fd44b290358",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "store = {}\n",
    "\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "\n",
    "with_message_history = RunnableWithMessageHistory(model, get_session_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c658d610-7aaa-4839-8bd5-4c9d297b7366",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"session_id\": \"abc2\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "af3461cc-7f08-4a42-abed-e0da97dffa45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in RootListenersTracer.on_llm_end callback: KeyError('message')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"AI: Hi Bob! It's nice to meet you. Is there something I can help you with or would you like to chat?\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Hi! I'm Bob\")],\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dcd2269d-2feb-4ef0-a59e-b4d7294f3918",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in RootListenersTracer.on_llm_end callback: KeyError('message')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I'm happy to help! Since we just started our conversation, I don't have any information about your name yet. Would you like to introduce yourself and tell me what your name is?\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"What's my name?\")],\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261dc155-1091-4323-8773-0fbad39c5799",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199b3a10-d1c1-40e8-9b27-c3c87382f009",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
